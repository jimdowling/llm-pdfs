{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://pythonhosted.org/PyDrive/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade\n",
    "!pip install pydrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LoadCredentialsFile(\"credentials.json\")\n",
    "if gauth.credentials is None:\n",
    "    gauth.LocalWebserverAuth()\n",
    "elif gauth.access_token_expired:\n",
    "    gauth.Refresh()\n",
    "else:\n",
    "    # Initialize the saved creds\n",
    "    gauth.Authorize()\n",
    "# Save the current credentials to a file\n",
    "gauth.SaveCredentialsFile(\"credentials.json\")\n",
    "\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file from GDrive in the folder \"slides\" (with id folder_id)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# folder_id = os.environ[\"FOLDER_ID\"]\n",
    "folder_id = \"1KfIKLn3sNzaxlwkjlnQczDsZa_2ohIQV\"\n",
    "\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList() \n",
    "new_files = []\n",
    "for file in file_list:\n",
    "    if not os.path.isfile(f\"{file['title']}\"): \n",
    "        print(\"title: %s, id: %s\" % (file[\"title\"],file[\"id\"]))\n",
    "        file.GetContentFile(file[\"title\"])\n",
    "        new_files.append(file)\n",
    "new_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# def load_split_pdf(pdf_path):\n",
    "#     pdf_loader = PyPDF2.PdfReader(open(pdf_path, \"rb\"))\n",
    "#     pdf_text = \"\"\n",
    "#     for page_num in range(len(pdf_loader.pages)):\n",
    "#         pdf_page = pdf_loader.pages[page_num]\n",
    "#         pdf_text += pdf_page.extract_text()\n",
    "#     progressBar(2, 7)\n",
    "#     return pdf_text\n",
    "\n",
    "transcriptions = [] \n",
    "embeddings = {}\n",
    "\n",
    "for file in new_files:\n",
    "\n",
    "    pdfReader = PyPDF2.PdfReader(file[\"title\"])\n",
    "\n",
    "    count = len(pdfReader.pages)\n",
    "    output = \"\"\n",
    "    embeddings[file[\"title\"]] = {\"text\":[]}\n",
    "    for i in range(count):\n",
    "        pageObj = pdfReader.pages[i]       \n",
    "        extr = pageObj.extract_text()\n",
    "        embeddings[file[\"title\"]][\"text\"].append(extr)\n",
    "        output += \"\\n\" + extr\n",
    "        \n",
    "    transcriptions.append(output)\n",
    "print(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "api_key = \"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\"\n",
    "#os.environ[\"OPENAI_API_KEY\"]\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "responses = []\n",
    "\n",
    "for t in transcriptions:\n",
    "    context = t\n",
    "    i = 0\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(ceil(len(t)/4097)):\n",
    "        chunks.append(t[i*4097:i*4097+4097])\n",
    "\n",
    "    for c in chunks:\n",
    "        context = c\n",
    "        question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 2 to 3 meaningful questions on the text and the respective answers. Plese reply in the JSON format {'questions':<questions generated>,'answers':<answers generated>}. DO NOT write anything else than the requested JSON and remember to write the full elaborated content and not just one part.\"\n",
    "        #question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 3 meaningful questions based on the new text and the respective answers. As for the reply, follow the following template FOR EACH pair of question and the respective answer: '[INST] <question> [/INST] <answer>'  and so on, let's call this template a 'block'.  NEVER use newlines other than separating blocks and NEVER write anything that is not formatted as the proposed template. DO NOT write anything else than the requested blocks and make sure everything is formatted correctly.\"\n",
    "        # response = openai.Completion.create(\n",
    "        # engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"\\nContext: {context}\\nQuestion: {question}\"\n",
    "        # )\n",
    "        # answer = response.choices[0].text.strip()\n",
    "        # print(answer)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        responses.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate instruction set\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "row_result = {\"prompt\":[],\"questions\":[],\"answers\":[]}\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    try:\n",
    "        tmp = json.loads(r)\n",
    "        for j in range(len(tmp[\"questions\"])):\n",
    "            instr = f\"<s> [INST] {tmp['questions'][j]} [/INST] {tmp['answers'][j]} </s>\"\n",
    "            row_result[\"questions\"].append(tmp['questions'][j])\n",
    "            row_result[\"answers\"].append(tmp['answers'][j])\n",
    "            row_result[\"prompt\"].append(instr)\n",
    "            # print(instr)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "instructions = pd.DataFrame(row_result)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "for index, row in instructions.iterrows():\n",
    "    print(row[\"prompt\"])\n",
    "\n",
    "# print(instructions.iloc[0][\"Instructions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emb = {\"source\":[],\"page\":[],\"content\":[]}\n",
    "for e in embeddings:\n",
    "    for idx,t in enumerate(embeddings[e][\"text\"]):\n",
    "        emb[\"source\"].append(e)\n",
    "        emb[\"page\"].append(idx)\n",
    "        emb[\"content\"].append(t)\n",
    "\n",
    "\n",
    "embedding_df = pd.DataFrame(emb)\n",
    "\n",
    "print(embedding_df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()\n",
    "emb_fg = fs.get_or_create_feature_group(\n",
    "    name=\"embeddings\",\n",
    "    version=1,\n",
    "    primary_key=list(embedding_df), \n",
    "    description=\"Content of each page of each file\")\n",
    "emb_fg.insert(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()\n",
    "instructions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"instructionset\",\n",
    "    version=4,\n",
    "    primary_key=list(instructions), \n",
    "    description=\"Instruction Set for fine tuning of llms\")\n",
    "instructions_fg.insert(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(embedding_df['source'].isin(['02_serverless_ml.pdf','Python']))\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
